{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate F-score given a gold and predicted list (or nbest predicted list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cc_utils\n",
    "import importlib\n",
    "importlib.reload(cc_utils)\n",
    "from cc_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = load_gold(\"/Users/hsajjad/Work/work/comparable-corpus/fr-en.training.gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_f = \"/Users/hsajjad/Work/work/comparable-corpus/predictions-1best\"\n",
    "pred = load_prediction(pred_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.023474109330057304, r = 0.7023992956196347, f = 0.04542995444191344\n"
     ]
    }
   ],
   "source": [
    "# calculate precision, recall and fmeasure\n",
    "score = calculateScore(pred, gold)\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calculate score of nbest list. This is to look at what is the best recall in our nbest list\n",
    "pred_nbest = nbest_oracle(\"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.027854815098170477, r = 0.8334800792427911, f = 0.053908029612756266\n"
     ]
    }
   ],
   "source": [
    "nbest = 10\n",
    "nbest_score = calculateOracleScore(pred_nbest, gold, nbest)\n",
    "print (nbest_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load machine translation outputs to find the best candidate sentences from a list of nbest candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270730"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load MT output\n",
    "### Format of file is French sentence tab English translation\n",
    "\n",
    "mt_pairs = {}\n",
    "mt_idx = {}\n",
    "\n",
    "train_mt=\"/Users/hsajjad/Work/work/comparable-corpus/mt/train-fr-en-mt-out\" \n",
    "load_mt(train_mt, mt_pairs, mt_idx)\n",
    "len(mt_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MT output for train\n",
    "### Format of file is French sentence tab English translation\n",
    "\n",
    "train_mt=\"/Users/hsajjad/Work/work/comparable-corpus/mt/train-fr-en-mt-out\"\n",
    "train_mt_pairs = {}\n",
    "train_mt_idx = {}\n",
    "load_mt(train_mt, train_mt_pairs, train_mt_idx)\n",
    "\n",
    "### save nbest file with bleu scores on every line\n",
    "train_nbest = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates\"\n",
    "train_nbest_bleu = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-bleu\"\n",
    "nbest_bleu_save(train_nbest, train_nbest_bleu, train_mt_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file with bleu score and choose the candidate with the hightest bleu\n",
    "train_nbest_bleu = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-bleu\"\n",
    "dic = getBestBleu(train_nbest_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p = 0.7902658089573977, r = 0.7165969623596742, f = 0.7516305916305915'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select candidates with bleu greater than a certain threshold\n",
    "threshold = 0.55\n",
    "train_final = filter_on_bleu(dic, threshold)\n",
    "calculateScore(train_final, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275683\n"
     ]
    }
   ],
   "source": [
    "# load MT output for test\n",
    "test_mt=\"/Users/hsajjad/Work/work/comparable-corpus/mt/test-fr-en-mt-out\"\n",
    "test_mt_pairs = {}\n",
    "test_mt_idx = {}\n",
    "load_mt(test_mt, test_mt_pairs, test_mt_idx)\n",
    "print (len(test_mt_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save nbest file with bleu scores on every line\n",
    "test_nbest = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-testset\"\n",
    "test_nbest_bleu = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-testset-bleu\"\n",
    "nbest_bleu_save(test_nbest, test_nbest_bleu, test_mt_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276833"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nbest_bleu = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-testset-bleu\"\n",
    "test_pred = getBestBleu(test_nbest_bleu)\n",
    "len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.52\n",
    "test_final = filter_on_bleu(test_pred, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some code to write files in BUCC submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bucc_format(st, tag):\n",
    "    st_rest = 9 - len(st)\n",
    "    for i in range(st_rest):\n",
    "        st = '0' + st\n",
    "    st = tag + st\n",
    "    return st\n",
    "        \n",
    "def output_format(pred_dic):\n",
    "    format_dic = {}\n",
    "    for key,value in pred_dic.items():\n",
    "        fr, en = key.split(\"\\t\")\n",
    "        fr = bucc_format(fr, \"fr-\")\n",
    "        en = bucc_format(en, \"en-\")\n",
    "        format_dic[fr + \"\\t\" + en] = 0\n",
    "    return format_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatted_out = output_format(test_final)\n",
    "test_output = open (\"/Users/hsajjad/Work/work/comparable-corpus/submission/test-embedding-mt-0.52\", 'w', encoding='utf-8')\n",
    "for key,value in formatted_out.items():\n",
    "    test_output.write(key)\n",
    "    test_output.write(\"\\n\")\n",
    "test_output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### QE task\n",
    "qeFile = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-qe-predictions-train.sentids+qescores-filtered\"\n",
    "qeFile = \"/Users/hsajjad/Work/work/comparable-corpus/eval-qe-bleu\"\n",
    "qeFile = \"/Users/hsajjad/Work/work/comparable-corpus/eval-qe-0.5\"\n",
    "\n",
    "qpred ={}\n",
    "\n",
    "qf = open(qeFile, 'r', encoding=\"utf-8\")\n",
    "for line in qf:\n",
    "    line = line.rstrip()\n",
    "    fr_id, en_id = line.split(\"\\t\")\n",
    "    fr_id = str(int(fr_id.replace('fr-','')))\n",
    "    en_id = str(int(en_id.replace('en-','')))\n",
    "    qpred[fr_id + \"\\t\" + en_id] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5894 2845 3192\n",
      "0.6744478773315025 0.6486902927580893 0.6613183730715287\n"
     ]
    }
   ],
   "source": [
    "calculateScore(qpred, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatted_out = output_format(test_final)\n",
    "test_output = open (\"/Users/hsajjad/Work/work/comparable-corpus/submission/test-embedding-qe\", 'w', encoding='utf-8')\n",
    "for key,value in formatted_out.items():\n",
    "    test_output.write(key)\n",
    "    test_output.write(\"\\n\")\n",
    "test_output.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "import numpy\n",
    "import utils\n",
    "\n",
    "# random\n",
    "#from random import shuffle\n",
    "# classifier\n",
    "#from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "en = np.loadtxt(open(\"/Users/hsajjad/Work/work/comparable-corpus/script/en-europarl.csv\", \"rb\"), delimiter=\",\")\n",
    "en=np.nan_to_num(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (en.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fr = np.loadtxt(open(\"/Users/hsajjad/Work/work/comparable-corpus/script/fr-europarl.csv\", \"rb\"), delimiter=\",\")\n",
    "fr=np.nan_to_num(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (fr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fr_chunk = fr[0:1]\n",
    "en_chunk = en[0:21000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "result = cosine_similarity(fr_chunk, en_chunk)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fr_chunk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Change chunk_size to control resource consumption and speed\n",
    "# Higher chunk_size means more memory/RAM needed but also faster \n",
    "chunk_size = 500 \n",
    "matrix_len = en.shape[0] # Not sparse numpy.ndarray\n",
    "\n",
    "def similarity_cosine_by_chunk(start, end):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return cosine_similarity(X=en[start:end], Y=fr) # scikit-learn function\n",
    "\n",
    "i =0\n",
    "\n",
    "for chunk_start in range(0, matrix_len, chunk_size):\n",
    "    f = \"output\" + \"-\" + str(i) + \".csv\"\n",
    "    \n",
    "    cosine_similarity_chunk = similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size)\n",
    "    print (cosine_similarity_chunk.shape)\n",
    "    #np.savetxt(f, cosine_similarity_chunk, delimiter = ',')\n",
    "    print (type(cosine_similarity_chunk))\n",
    "    i +=1\n",
    "    \n",
    "    \n",
    "    # Handle cosine_similarity_chunk  ( Write it to file_timestamp and close the file )\n",
    "    # Do not open the same file again or you may end up with out of memory after few chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy import genfromtxt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.spatial as sp\n",
    "import sys\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the file with cosine similarity scores\n",
    "a=np.load('/Users/hsajjad/Work/work/comparable-corpus/output-0.csv.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66106670517620991"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape\n",
    "print (np.argmax(a[0]))\n",
    "a[2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=1\n",
    "sorted_idx = np.argsort(-a) # get the sorted index ascending order based on values in the array\n",
    "top_idx = sorted_idx[:,:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 271874)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function load predicted pairs from the embedding matrix , one best pair\n",
    "def pred_pairs(top_idx):\n",
    "    pred_pair = {}\n",
    "    for i in range(top_idx.shape[0]):\n",
    "            src_sent = i + 1\n",
    "            tgt_sent = top_idx[i,0] + 1\n",
    "            pred_pair [str(src_sent) + \"\\t\" + str(tgt_sent)] = 0\n",
    "    return pred_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_prediction(fname):\n",
    "    fin = open (fname, 'r', encoding='utf-8')\n",
    "    pred = {}\n",
    "    for line in fin:\n",
    "        line = line.rstrip()\n",
    "        pred[line] = 0 # loading french english pairs\n",
    "    return (pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_gold(fname):\n",
    "    fin = open (fname, 'r', encoding='utf-8')\n",
    "    gold = {}\n",
    "    for line in fin:\n",
    "        fr, en = line.split(\"\\t\")\n",
    "        fr_int = int(fr.replace('fr-',''))\n",
    "        en_int = int(en.replace('en-',''))\n",
    "    \n",
    "        gold[str(fr_int) + \"\\t\" + str(en_int)] = 0 # making french english pairs\n",
    "    return (gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_gold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-33cead94e90f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_gold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/hsajjad/Work/work/comparable-corpus/fr-en.training.gold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_gold' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### this is need only when predictions are in full matrix form\n",
    "predicted_pairs = pred_pairs(top_idx)\n",
    "print (len(predicted_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cc_utils\n",
    "import importlib\n",
    "importlib.reload(cc_utils)\n",
    "from cc_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = load_gold(\"/Users/hsajjad/Work/work/comparable-corpus/fr-en.training.gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_f = \"/Users/hsajjad/Work/work/comparable-corpus/predictions-1best\"\n",
    "pred = load_prediction(pred_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.023474109330057304, r = 0.7023992956196347, f = 0.04542995444191344\n"
     ]
    }
   ],
   "source": [
    "# calculate precision, recall and fmeasure\n",
    "score = calculateScore(pred, gold)\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to calculate score of nbest list. This is to look at what is the best recall in our nbest list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_nbest = nbest_oracle(\"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.027854815098170477, r = 0.8334800792427911, f = 0.053908029612756266\n"
     ]
    }
   ],
   "source": [
    "nbest_score = calculateOracleScore(pred_nbest, gold, 10)\n",
    "print (nbest_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2718740"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_nbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### load MT file ## French to English\n",
    "def load_mt(fname, mt_pairs, mt_idx):\n",
    "    fin = open (fname, 'r', encoding='utf-8')\n",
    "    count = 1\n",
    "    for line in fin:\n",
    "        fr, en = line.split(\"\\t\")\n",
    "        mt_pairs[fr] = en\n",
    "        mt_idx[count] = fr\n",
    "        count += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270730"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### loading MT output\n",
    "\n",
    "mt_pairs = {}\n",
    "mt_idx = {}\n",
    "train_mt=\"/Users/hsajjad/Work/work/comparable-corpus/mt/train-fr-en-mt-out\"\n",
    "load_mt(train_mt, mt_pairs, mt_idx)\n",
    "len(mt_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyter\n",
    "def compareNbest_with_mt_ter(fr, en, mt_pairs):\n",
    "    mt_en = mt_pairs[fr].split() #get corresponding translation\n",
    "    en_wrd = en.split()\n",
    "    return pyter.ter(en_wrd, mt_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyter\n",
    "def compareNbest_with_mt(fr, en, mt_pairs):\n",
    "    mt_en = mt_pairs[fr] #get corresponding translation\n",
    "    return nltk.translate.bleu_score.sentence_bleu([mt_en], en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### take nbest file and add an extra column with bleu score\n",
    "\n",
    "def nbest_bleu_save(in_file, out_file, mt_pairs):\n",
    "    \n",
    "    nbest_out = open (out_file, 'w', encoding='utf-8')\n",
    "    nbest = open (in_file, 'r', encoding='utf-8')\n",
    "\n",
    "    curr_bleu = 0\n",
    "    for line in nbest:\n",
    "        line = line.rstrip()\n",
    "        fr_id, fr, en_id, en = line.split(\"\\t\")\n",
    "\n",
    "        curr_bleu = compareNbest_with_mt(fr, en, mt_pairs)\n",
    "\n",
    "        nbest_out.write (line + \"\\t\" + str(curr_bleu) + \"\\n\")\n",
    "    nbest_out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### take nbest file and add an extra column with bleu score\n",
    "\n",
    "def nbest_ter_save(in_file, out_file, mt_pairs):\n",
    "    \n",
    "    nbest_out = open (out_file, 'w', encoding='utf-8')\n",
    "    nbest = open (in_file, 'r', encoding='utf-8')\n",
    "\n",
    "    curr_ter = 0\n",
    "    for line in nbest:\n",
    "        line = line.rstrip()\n",
    "        fr_id, fr, en_id, en = line.split(\"\\t\")\n",
    "\n",
    "        curr_ter = compareNbest_with_mt_ter(fr, en, mt_pairs)\n",
    "\n",
    "        nbest_out.write (line + \"\\t\" + str(curr_ter) + \"\\n\")\n",
    "    nbest_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-277-f8b7a4ee7005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_nbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_nbest_ter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-ter\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnbest_ter_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_nbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nbest_ter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mt_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-1f76571d46e9>\u001b[0m in \u001b[0;36mnbest_ter_save\u001b[0;34m(in_file, out_file, mt_pairs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mfr_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcurr_ter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompareNbest_with_mt_ter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmt_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnbest_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_ter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-275-b742c417135a>\u001b[0m in \u001b[0;36mcompareNbest_with_mt_ter\u001b[0;34m(fr, en, mt_pairs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmt_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get corresponding translation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0men_wrd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_wrd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmt_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/hsajjad/anaconda/lib/python3.5/site-packages/pyter/__init__.py\u001b[0m in \u001b[0;36mter\u001b[0;34m(inputwords, refwords)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0minputwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0med\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCachedEditDistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hsajjad/anaconda/lib/python3.5/site-packages/pyter/__init__.py\u001b[0m in \u001b[0;36m_ter\u001b[0;34m(iwords, rwords, mtd)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# print('[ED]', mtd(iwords))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_iwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# print('[I]', u' '.join(iwords))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# print('[R]', u' '.join(rwords))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hsajjad/anaconda/lib/python3.5/site-packages/pyter/__init__.py\u001b[0m in \u001b[0;36m_shift\u001b[0;34m(iwords, rwords, mtd)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0mshift\u001b[0m \u001b[0moccurred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \"\"\"\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpre_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0misp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_findpairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hsajjad/anaconda/lib/python3.5/site-packages/pyter/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iwords)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mstart_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewly_created_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewly_created_matrix\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# もう一度たどって、キャッシュがないノードにキャッシュを挿入していく\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hsajjad/anaconda/lib/python3.5/site-packages/pyter/__init__.py\u001b[0m in \u001b[0;36m_edit_distance\u001b[0;34m(self, iwords, spos, cache)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mspos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mspos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrwds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load MT output for train\n",
    "train_mt=\"/Users/hsajjad/Work/work/comparable-corpus/mt/train-fr-en-mt-out\"\n",
    "train_mt_pairs = {}\n",
    "train_mt_idx = {}\n",
    "load_mt(train_mt, train_mt_pairs, train_mt_idx)\n",
    "\n",
    "### save nbest file with bleu scores on every line\n",
    "train_nbest = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates\"\n",
    "train_nbest_ter = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-ter\"\n",
    "nbest_ter_save(train_nbest, train_nbest_ter, train_mt_pairs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hsajjad/anaconda/lib/python3.5/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/Users/hsajjad/anaconda/lib/python3.5/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/Users/hsajjad/anaconda/lib/python3.5/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "train_nbest_ter = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-ter\"\n",
    "dic = getBestBleu(train_nbest_ter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### load nbest file with bleu and filter out pairs\n",
    "import sys\n",
    "\n",
    "def getBestBleu(file_train_nbest_bleu):\n",
    "    \n",
    "    nbest_bleu = open (file_train_nbest_bleu, 'r', encoding='utf-8')\n",
    "\n",
    "    best_bleu = -1\n",
    "    best_str = \"\"\n",
    "    dic = {}\n",
    "\n",
    "    for line in nbest_bleu:\n",
    "        line = line.rstrip()\n",
    "        fr_id, fr, en_id, en, bleu = line.split(\"\\t\")\n",
    "\n",
    "        if fr_id not in dic:\n",
    "            dic[fr_id] = line\n",
    "            best_bleu = bleu\n",
    "            best_str = line\n",
    "        elif bleu > best_bleu:\n",
    "            best_bleu = bleu\n",
    "            best_str = line\n",
    "            dic[fr_id] = best_str\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### get best bleu dictionary and threshold on bleu\n",
    "\n",
    "def filter_on_bleu(dic, threshold):\n",
    "    final_list = {}\n",
    "    eval_list = {}\n",
    "    for key,value in dic.items():\n",
    "        fr_id, fr, en_id, en, bleu = value.split(\"\\t\")\n",
    "        if float(bleu) > threshold:\n",
    "            final_list[key] = value\n",
    "            fr_id = str(int(fr_id.replace('fr-','')))\n",
    "            en_id = str(int(en_id.replace('en-','')))\n",
    "            eval_list[fr_id + \"\\t\" + en_id] = float(bleu)\n",
    "    return eval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load MT output for train\n",
    "train_mt=\"/Users/hsajjad/Work/work/comparable-corpus/mt/train-fr-en-mt-out\"\n",
    "train_mt_pairs = {}\n",
    "train_mt_idx = {}\n",
    "load_mt(train_mt, train_mt_pairs, train_mt_idx)\n",
    "\n",
    "### save nbest file with bleu scores on every line\n",
    "train_nbest = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates\"\n",
    "train_nbest_bleu = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-bleu\"\n",
    "nbest_bleu_save(train_nbest, train_nbest_bleu, train_mt_pairs)\n",
    "\n",
    "train_nbest_bleu = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-bleu\"\n",
    "dic = getBestBleu(train_nbest_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477 264396 1609\n",
      "0.027501811507578907 0.8229143737618314 0.053224847753586824\n"
     ]
    }
   ],
   "source": [
    "threshold = 0\n",
    "train_final = filter_on_bleu(dic, threshold)\n",
    "calculateScore(train_final, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275683\n"
     ]
    }
   ],
   "source": [
    "# load MT output for test\n",
    "test_mt=\"/Users/hsajjad/Work/work/comparable-corpus/mt/test-fr-en-mt-out\"\n",
    "test_mt_pairs = {}\n",
    "test_mt_idx = {}\n",
    "load_mt(test_mt, test_mt_pairs, test_mt_idx)\n",
    "print (len(test_mt_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save nbest file with bleu scores on every line\n",
    "test_nbest = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-testset\"\n",
    "test_nbest_bleu = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-testset-bleu\"\n",
    "nbest_bleu_save(test_nbest, test_nbest_bleu, test_mt_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276833"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nbest_bleu = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-parallel-top10-candidates-testset-bleu\"\n",
    "test_pred = getBestBleu(test_nbest_bleu)\n",
    "len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.52\n",
    "test_final = filter_on_bleu(test_pred, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bucc_format(st, tag):\n",
    "    st_rest = 9 - len(st)\n",
    "    for i in range(st_rest):\n",
    "        st = '0' + st\n",
    "    st = tag + st\n",
    "    return st\n",
    "        \n",
    "def output_format(pred_dic):\n",
    "    format_dic = {}\n",
    "    for key,value in pred_dic.items():\n",
    "        fr, en = key.split(\"\\t\")\n",
    "        fr = bucc_format(fr, \"fr-\")\n",
    "        en = bucc_format(en, \"en-\")\n",
    "        format_dic[fr + \"\\t\" + en] = 0\n",
    "    return format_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatted_out = output_format(test_final)\n",
    "test_output = open (\"/Users/hsajjad/Work/work/comparable-corpus/submission/test-embedding-mt-0.52\", 'w', encoding='utf-8')\n",
    "for key,value in formatted_out.items():\n",
    "    test_output.write(key)\n",
    "    test_output.write(\"\\n\")\n",
    "test_output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6511 1728 2575\n",
      "0.7902658089573977 0.7165969623596742 0.7516305916305915\n"
     ]
    }
   ],
   "source": [
    "calculateScore(eval_list, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8239"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref = open (\"/Users/hsajjad/Desktop/ref.ar\", 'r', encoding='utf-8')\n",
    "hypothesis = []\n",
    "for line in ref:\n",
    "    line = line.strip('\\n')\n",
    "    hypothesis.append(line.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyp = open(\"/Users/hsajjad/Desktop/output-ar\", 'r', encoding='utf-8')\n",
    "reference = []\n",
    "for line in hyp:\n",
    "    line = line.strip('\\n')\n",
    "    reference.append(line.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-8e1fd5dae455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSmoothingFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "cc = SmoothingFunction()\n",
    "for i in range(len(hypothesis)):\n",
    "    print(nltk.translate.bleu_score.sentence_bleu([reference[i]], hypothesis[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "par = \"This is great. I am working.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_tokenize_list = sent_tokenize(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'great.']\n",
      "['I', 'am', 'working.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sent_tokenize_list)):\n",
    "    sent = sent_tokenize_list[i]\n",
    "    words = print (sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyter' from '/Users/hsajjad/anaconda/lib/python3.5/site-packages/pyter/__init__.py'>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyter\n",
    "import importlib\n",
    "importlib.reload(pyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp=\"this is going great\".split()\n",
    "ref=\"this is awesome what do you thing about it. we will enjoy this work\".split()\n",
    "pyter.ter(hyp, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### QE task\n",
    "qeFile = \"/Users/hsajjad/Work/work/comparable-corpus/fr-en-qe-predictions-train.sentids+qescores-filtered\"\n",
    "qeFile = \"/Users/hsajjad/Work/work/comparable-corpus/eval-qe-bleu\"\n",
    "qeFile = \"/Users/hsajjad/Work/work/comparable-corpus/eval-qe-0.5\"\n",
    "\n",
    "qpred ={}\n",
    "\n",
    "qf = open(qeFile, 'r', encoding=\"utf-8\")\n",
    "for line in qf:\n",
    "    line = line.rstrip()\n",
    "    fr_id, en_id = line.split(\"\\t\")\n",
    "    fr_id = str(int(fr_id.replace('fr-','')))\n",
    "    en_id = str(int(en_id.replace('en-','')))\n",
    "    qpred[fr_id + \"\\t\" + en_id] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8739"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5894 2845 3192\n",
      "0.6744478773315025 0.6486902927580893 0.6613183730715287\n"
     ]
    }
   ],
   "source": [
    "calculateScore(qpred, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatted_out = output_format(test_final)\n",
    "test_output = open (\"/Users/hsajjad/Work/work/comparable-corpus/submission/test-embedding-qe\", 'w', encoding='utf-8')\n",
    "for key,value in formatted_out.items():\n",
    "    test_output.write(key)\n",
    "    test_output.write(\"\\n\")\n",
    "test_output.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
